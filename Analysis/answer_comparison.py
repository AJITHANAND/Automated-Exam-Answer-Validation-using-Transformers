# -*- coding: utf-8 -*-
"""Answer Comparison.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xXYV7vzLM1gd1MCBs1qIAuY226PoAMLw
"""

correct_answer = "an array is a data structure that stores a collection of values, all of the same type,in contiguous memory locations. Each value in an array is identified by an index or a subscript,which is a non-negative integer that represents its position in the array. The index of the first element in an array is typically 0, and the index of the last element is usually one less than the size of the array."
answer = "Array is a collection of homogeneous data.It is continuous in memory"

import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tag import pos_tag
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from string import punctuation

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

data = pd.DataFrame({'text': [correct_answer, answer]})
print(data)
# data = pd.read_csv('dataset/sample dataset.csv')
stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()
docs = []
word_freqs = {}
idx = 0
for doc in data['text']:
    # Tokenization
    tokens = word_tokenize(doc.lower())
    # Part-of-Speech Tagging
    tagged_tokens = pos_tag(tokens)
    # Stopword, preposition, and punctuation removal
    filtered_tokens = [word for word, pos in tagged_tokens if
                       word not in stop_words and pos != 'IN' and word not in punctuation]
    # Stemming
    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]
    # Combine stemmed tokens back into documents
    word_freq = {}
    for word in stemmed_tokens:
        if word in word_freq:
            word_freq[word] += 1
        else:
            word_freq[word] = 1
        # Create indexing
        if word not in word_freqs:
            word_freqs[word] = idx
            idx += 1
    docs.append(dict(sorted(word_freq.items())))

print(docs)

# Create the vector space model
num_docs = len(docs)
num_words = len(word_freqs)
vectors = []
for i in range(num_docs):
    doc_freqs = docs[i]
    vector = [0] * num_words
    for word, freq in doc_freqs.items():
        idx = word_freqs[word]
        vector[idx] = freq
    vectors.append(vector)

# Sort the vectors in ascending order of the words
vocabulary = sorted(word_freqs.keys())
sorted_vectors = []
for vector in vectors:
    sorted_vector = [vector[word_freqs[word]] for word in vocabulary]
    sorted_vectors.append(sorted_vector)

print(sorted_vectors)

# Compute inverse document frequency
import math

idf = {}
for word, idx in word_freqs.items():
    df = sum([1 for doc in docs if word in doc])
    idf[word] = math.log(num_docs / df)
print(idf)

# Compute the weights of the words
for i in range(num_docs):
    doc_freqs = docs[i]
    vector = vectors[i]
    for word, freq in doc_freqs.items():
        idx = word_freqs[word]
        tf_idf = freq * idf[word]
        vector[idx] = tf_idf
    vectors[i] = vector
print(vectors)

# Normalize all documents to unit length
for i in range(num_docs):
    vector = vectors[i]
    norm = math.sqrt(sum([x ** 2 for x in vector]))
    vector = [x / norm for x in vector]
    vectors[i] = vector
print(vectors)

# Find cosine similarity between documents
cos_sim = cosine_similarity([vectors[0]], [vectors[1]])
print('Cosine similarity between documents 0 and 1:', cos_sim[0][0])
